{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hydrostats as hs\n",
    "import hydrostats.data as hd\n",
    "import HydroErr as he\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the main CSV file\n",
    "main_df = pd.read_parquet('parquet that contain the gauge number and the geoglows river id')\n",
    "main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dates(date_series):\n",
    "    dt = pd.to_datetime(date_series, format=\"%Y-%m-%d\", errors='coerce')\n",
    "    dt = dt.fillna(pd.to_datetime(date_series, format=\"%m/%d/%y\", errors='coerce'))\n",
    "    dt = dt.apply(lambda x: x if pd.isnull(x) or x.year <= 2025 else x - pd.DateOffset(years=100))\n",
    "    return dt\n",
    "\n",
    "def process_row(row, missing_files):\n",
    "    matching_column = row['gauge_id']   # change the column that represent the observed gauged column\n",
    "    validation = row['model_id']    # change the column that represent either GEOGLOWS V1 or V2 column\n",
    "\n",
    "    matching_file_path = f'/Users/yubinbaaniya/Documents/WORLD BIAS/saber workdir/gauge_data/{matching_column}.csv'  # path to folder where you have observed gauge\n",
    "    validation_file_path = f'/Users/yubinbaaniya/Documents/WORLD BIAS/saber workdir/Bias corrected Time series/Jorge/{validation}.csv'  # path to individual simulated value\n",
    " \n",
    "    # Check if both files exist\n",
    "    if not os.path.exists(matching_file_path):\n",
    "        missing_files.append(matching_file_path)\n",
    "        return None\n",
    "    if not os.path.exists(validation_file_path):\n",
    "        missing_files.append(validation_file_path)\n",
    "        return None\n",
    "\n",
    "    # Load the corresponding CSV files using pandas and rename columns\n",
    "    df_o = pd.read_csv(matching_file_path, dtype={'Date': 'str'}, names=['Date', 'Observed'], skiprows=1)\n",
    "    df_s = pd.read_csv(validation_file_path, dtype={'Date': 'str'}, names=['Date', 'Simulated'], skiprows=1)\n",
    " \n",
    "    # Parse dates\n",
    "    df_o['Date'] = parse_dates(df_o['Date'])\n",
    "    df_s['Date'] = parse_dates(df_s['Date'])\n",
    "\n",
    "    # Merge data and set the parsed date as the index\n",
    "    merged_df = pd.merge(df_o, df_s, on='Date', how='inner').set_index('Date')\n",
    "\n",
    "    merged_df = merged_df.sort_index()\n",
    "\n",
    "    # Filter data by date range\n",
    "    merged_df = merged_df['1940-01-01':'2024-08-08']\n",
    "\n",
    "    # Convert columns to numeric\n",
    "    merged_df = merged_df.apply(pd.to_numeric, errors='coerce')\n",
    "    #merged_df.to_csv(f'/Users/yubinbaaniya/Documents/Bias Correction/GOOGLE/m-dat/bias corrected/{matching_column}.csv')\n",
    "    # Print statement to indicate which merged_df is being saved\n",
    "    print(f\"Processed and merged data for {matching_column}\")\n",
    "\n",
    "    # Filter out negative values\n",
    "    merged_df_filtered = merged_df[(merged_df['Simulated'] >= 0) & (merged_df['Observed'] >= 0)]\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'me': he.me(merged_df['Simulated'], merged_df['Observed'], remove_neg=True),\n",
    "        'rmse': he.rmse(merged_df['Simulated'], merged_df['Observed'], remove_neg=True),\n",
    "        'mae': he.mae(merged_df['Simulated'], merged_df['Observed'], remove_neg=True),\n",
    "        'nse': he.nse(merged_df['Simulated'], merged_df['Observed'], remove_neg=True),\n",
    "        'pearson_r': he.pearson_r(merged_df['Simulated'], merged_df['Observed'], remove_neg=True),\n",
    "        'r_squared': he.r_squared(merged_df['Simulated'], merged_df['Observed'], remove_neg=True),\n",
    "        'kge_2012': he.kge_2012(merged_df['Simulated'], merged_df['Observed'], remove_neg=True),\n",
    "        'nrmse_mean': he.nrmse_mean(merged_df['Simulated'], merged_df['Observed'], remove_neg=True),\n",
    "        'std_obs': merged_df_filtered['Observed'].std(),\n",
    "        'std_sim': merged_df_filtered['Simulated'].std(),\n",
    "        'mean_obs': merged_df_filtered['Observed'].mean(),\n",
    "        'mean_sim': merged_df_filtered['Simulated'].mean(),\n",
    "        'name': f\"{matching_column}\"\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    num_jobs = os.cpu_count()  # Number of cores to use\n",
    "\n",
    "    missing_files = []\n",
    "\n",
    "    # Use joblib for parallel processing\n",
    "    results = Parallel(n_jobs=num_jobs)(\n",
    "        delayed(process_row)(row, missing_files) for index, row in main_df.iterrows()\n",
    "    )\n",
    "\n",
    "    # Filter out None results\n",
    "    filtered_results = [result for result in results if result is not None]\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    results_df = pd.DataFrame(filtered_results)\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    results_df.to_csv('patah to save the performance metrics', index=False)\n",
    "\n",
    "    # Save the missing files report\n",
    "    #with open('/Users/yubin/Library/CloudStorage/Box-Box/fRANCE_/v2_files.txt', 'w') as f:\n",
    "        #for file in missing_files:\n",
    "            #f.write(f\"{file}\\n\")\n",
    "\n",
    "    print(\"Results saved to metrics.csv\")\n",
    "    print(\"Missing files report saved to missing_files.txt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
