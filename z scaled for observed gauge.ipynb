{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from joblib import Parallel, delayed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20553 CSV files in /Users/yubinbaaniya/Documents/WORLD BIAS/saber workdir/gauge_data. Starting processing...\n",
      "Finished processing 20553 files. Summary saved to /Users/yubinbaaniya/Documents/WORLD BIAS/saber workdir/z_scaled_gauge_2nd_iteration.csv.\n"
     ]
    }
   ],
   "source": [
    "def calculate_z_scaled_fdc(df: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate the Z-scaled Flow Duration Curve (FDC) for a given DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame with 'Streamflow (m3/s)' column.\n",
    "\n",
    "    Returns:\n",
    "    - z_scaled: numpy array with Z-scaled streamflow values.\n",
    "    \"\"\"\n",
    "    # Ensure the 'Streamflow (m3/s)' column is numeric and drop any non-numeric entries\n",
    "    df['Streamflow (m3/s)'] = pd.to_numeric(df['Streamflow (m3/s)'], errors='coerce')\n",
    "    df = df.dropna(subset=['Streamflow (m3/s)'])\n",
    "\n",
    "    # Remove any values less than 0\n",
    "    df = df[df['Streamflow (m3/s)'] >= 0]\n",
    "\n",
    "    # Extract the streamflow data as a NumPy array\n",
    "    data_array = df['Streamflow (m3/s)'].values\n",
    "\n",
    "    if data_array.size == 0:\n",
    "        # If after cleaning, there are no data points, return an empty array\n",
    "        return np.full(101, np.nan)\n",
    "\n",
    "    # Define the percentiles in a gap of 1\n",
    "    percentiles = np.arange(100, -1, -1)\n",
    "\n",
    "    # Calculate the percentiles for the streamflow data\n",
    "    streamflow_percentiles = np.percentile(data_array, [p for p in percentiles])\n",
    "\n",
    "    # Z-scale the data (mean 0, std 1)\n",
    "    z_scaled = (streamflow_percentiles - np.mean(streamflow_percentiles)) / np.std(streamflow_percentiles)\n",
    "\n",
    "    return z_scaled\n",
    "\n",
    "def process_single_csv_file(csv_file_path: str) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Process a single CSV file: read, compute Z-scaled FDC, and return a Series.\n",
    "\n",
    "    Parameters:\n",
    "    - csv_file_path: Path to the input CSV file.\n",
    "\n",
    "    Returns:\n",
    "    - A pandas Series where the index is the percentiles (0 to 100) and the first element is the file name.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract the file name without extension\n",
    "        file_name = os.path.splitext(os.path.basename(csv_file_path))[0]\n",
    "\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "\n",
    "        # Calculate Z-scaled FDC\n",
    "        z_scaled_fdc = calculate_z_scaled_fdc(df)\n",
    "\n",
    "        # Create a Series with the filename and the Z-scaled FDC values\n",
    "        return pd.Series([file_name] + z_scaled_fdc.tolist())\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {csv_file_path}: {e}\")\n",
    "        return pd.Series()\n",
    "\n",
    "def process_all_csv_files_in_folder(folder_path: str, save_path: str, n_jobs: int = -1) -> None:\n",
    "    \"\"\"\n",
    "    Process all CSV files in the given folder in parallel, computing Z-scaled FDCs and saving a summary file.\n",
    "\n",
    "    Parameters:\n",
    "    - folder_path: Path to the folder containing input CSV files.\n",
    "    - save_path: Path to save the summary CSV file.\n",
    "    - n_jobs: Number of parallel jobs. Default is -1 (use all available cores).\n",
    "    \"\"\"\n",
    "    # Find all CSV files in the specified folder\n",
    "    csv_files = glob.glob(os.path.join(folder_path, '*.csv'))\n",
    "\n",
    "    if not csv_files:\n",
    "        print(f\"No CSV files found in the folder {folder_path}.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(csv_files)} CSV files in {folder_path}. Starting processing...\")\n",
    "\n",
    "    # Process files in parallel and collect results\n",
    "    results = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(process_single_csv_file)(csv_file_path)\n",
    "        for csv_file_path in csv_files\n",
    "    )\n",
    "\n",
    "    # Convert the list of Series to a DataFrame\n",
    "    summary_df = pd.DataFrame(results)\n",
    "\n",
    "    # Define column names: first column is 'File', others are 0, 1, 2, ..., 100\n",
    "    summary_df.columns = ['File'] + list(range(101))\n",
    "\n",
    "    # Save the summary DataFrame to a CSV file\n",
    "    summary_df.to_csv(save_path, index=False)\n",
    "\n",
    "    print(f\"Finished processing {len(csv_files)} files. Summary saved to {save_path}.\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == '__main__':\n",
    "    folder_path = '/Users/yubinbaaniya/Documents/WORLD BIAS/saber workdir/gauge_data'  # Replace with the path to your folder containing CSV files\n",
    "    save_path = '/Users/yubinbaaniya/Documents/WORLD BIAS/saber workdir/z_scaled_gauge_2nd_iteration.csv'  # Replace with the path to save the summary CSV file\n",
    "\n",
    "    # Process all CSV files and save the summary file\n",
    "    process_all_csv_files_in_folder(folder_path, save_path, n_jobs=-1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
