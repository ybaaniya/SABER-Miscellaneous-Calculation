{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from datetime import datetime\n",
    "import zarr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "obtain a parquet z scale fdc from all individual simulated csv to perform clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_z_scaled_fdc(data):\n",
    "    \"\"\"\n",
    "    Calculate z-scaled flow duration curve for a given dataset\n",
    "    \n",
    "    Args:\n",
    "        data (pandas.DataFrame): DataFrame with a 'streamflow_m^3/s' column\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with percentiles from Q0 to Q100\n",
    "    \"\"\"\n",
    "    # Extract streamflow values\n",
    "    streamflow = data['streamflow_m^3/s'].values\n",
    "    \n",
    "    # Z-scale the values\n",
    "    mean = np.mean(streamflow)\n",
    "    std = np.std(streamflow)\n",
    "    z_scaled = (streamflow - mean) / std\n",
    "    \n",
    "    # Sort in descending order (for FDC)\n",
    "    z_scaled_sorted = np.sort(z_scaled)[::-1]\n",
    "    \n",
    "    # Calculate percentiles in intervals of 1 (0 to 100)\n",
    "    percentiles = {}\n",
    "    n = len(z_scaled_sorted)\n",
    "    \n",
    "    for i in range(101):  # 0 to 100 inclusive\n",
    "        position = (i / 100) * (n - 1)\n",
    "        lower_index = int(np.floor(position))\n",
    "        upper_index = int(np.ceil(position))\n",
    "        \n",
    "        if lower_index == upper_index:\n",
    "            percentiles[f'Q{i}'] = z_scaled_sorted[lower_index]\n",
    "        else:\n",
    "            weight = position - lower_index\n",
    "            percentiles[f'Q{i}'] = z_scaled_sorted[lower_index] * (1 - weight) + z_scaled_sorted[upper_index] * weight\n",
    "    \n",
    "    return percentiles\n",
    "\n",
    "def process_csv_files(directory_path):\n",
    "    \"\"\"\n",
    "    Process all CSV files in a directory and calculate z-scaled FDCs\n",
    "    \n",
    "    Args:\n",
    "        directory_path (str): Path to directory containing CSV files\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame with z-scaled FDCs for all CSV files\n",
    "    \"\"\"\n",
    "    # Get list of all CSV files in the directory\n",
    "    csv_files = glob.glob(os.path.join(directory_path, '*.csv'))\n",
    "    \n",
    "    if not csv_files:\n",
    "        raise ValueError(f\"No CSV files found in {directory_path}\")\n",
    "    \n",
    "    # Process each file\n",
    "    results = []\n",
    "    for file_path in csv_files:\n",
    "        try:\n",
    "            # Read CSV file\n",
    "            df = pd.read_csv(file_path, parse_dates=['datetime'])\n",
    "            \n",
    "            # Calculate FDC\n",
    "            fdc = calculate_z_scaled_fdc(df)\n",
    "            \n",
    "            # Get file name without extension as index\n",
    "            file_index = os.path.splitext(os.path.basename(file_path))[0]\n",
    "            \n",
    "            # Add to results\n",
    "            results.append({'index': file_index, **fdc})\n",
    "            \n",
    "            print(f\"Processed: {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "    \n",
    "    # Create a dataframe from results\n",
    "    if not results:\n",
    "        raise ValueError(\"No valid results were obtained from the CSV files\")\n",
    "    \n",
    "    result_df = pd.DataFrame(results)\n",
    "    result_df.set_index('index', inplace=True)\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Directory containing CSV files\n",
    "    csv_dir = 'path to folder that contain individual csv simulated files'\n",
    "    \n",
    "    # Process all CSV files\n",
    "    print(f\"Processing CSV files in {csv_dir}...\")\n",
    "    result_df = process_csv_files(csv_dir)\n",
    "    \n",
    "    # Save to parquet\n",
    "    output_path = 'path to folder save the parquet files'\n",
    "    result_df.to_parquet(output_path)\n",
    "    \n",
    "    print(f\"Z-scaled FDC saved to {output_path}\")\n",
    "    print(f\"Processed {len(result_df)} files successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make a zarr file of individual simulated data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv_files_to_zarr(csv_dir, output_zarr_path):\n",
    "    \"\"\"\n",
    "    Process all CSV files in a directory and create a Zarr file with specified structure\n",
    "    \n",
    "    Args:\n",
    "        csv_dir (str): Path to directory containing CSV files\n",
    "        output_zarr_path (str): Path to save the Zarr file\n",
    "    \"\"\"\n",
    "    # Get list of all CSV files in the directory\n",
    "    csv_files = glob.glob(os.path.join(csv_dir, '*.csv'))\n",
    "    \n",
    "    if not csv_files:\n",
    "        raise ValueError(f\"No CSV files found in {csv_dir}\")\n",
    "    \n",
    "    print(f\"Found {len(csv_files)} CSV files\")\n",
    "    \n",
    "    # Get all unique river IDs from filenames (assuming filename is the river ID)\n",
    "    rivids = [int(os.path.splitext(os.path.basename(file))[0]) for file in csv_files]\n",
    "    \n",
    "    # specify the time range\n",
    "    time_index = pd.date_range(start='1980-01-01', end='2023-01-01', freq='D')\n",
    "    \n",
    "    print(f\"Creating dataset with {len(rivids)} river IDs and {len(time_index)} time steps\")\n",
    "    \n",
    "    # Create xarray dataset with the specified structure\n",
    "    ds = xr.Dataset(\n",
    "        coords={\n",
    "            'rivid': ('rivid', np.array(rivids, dtype=np.int32)),\n",
    "            'time': time_index,\n",
    "        },\n",
    "        attrs={\n",
    "            'Conventions': 'CF-1.6',\n",
    "            'comment': '',\n",
    "            'featureType': 'timeSeries',\n",
    "            'history': '',\n",
    "            'date_created': datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S+00:00\"),\n",
    "            'institution': '',\n",
    "            'references': 'https://github.com/c-h-david/rapid/, http://dx.doi.org/10.1175/2011JHM1345.1',\n",
    "            'source': 'RAPID: unknown, NOT a git repository, water inflow: /mnt/inflows/101/m3_101_20240731_20241016.nc',\n",
    "            'title': ''\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Initialize Qout data variable with NaN values\n",
    "    # Using chunking for better performance with large datasets\n",
    "    ds['Qout'] = xr.DataArray(\n",
    "        data=np.full((len(time_index), len(rivids)), np.nan, dtype=np.float32),\n",
    "        dims=['time', 'rivid'],\n",
    "        coords={'time': time_index, 'rivid': rivids},\n",
    "        attrs={\n",
    "            'long_name': 'River Discharge',\n",
    "            'units': 'm^3/s'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(\"Processing CSV files and populating dataset...\")\n",
    "    \n",
    "    # Process each file and populate dataset\n",
    "    for i, file_path in enumerate(csv_files):\n",
    "        try:\n",
    "            rivid = int(os.path.splitext(os.path.basename(file_path))[0])\n",
    "            rivid_index = rivids.index(rivid)\n",
    "            \n",
    "            # Read CSV file\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Convert datetime column to pandas datetime\n",
    "            # Try different date formats if needed\n",
    "            try:\n",
    "                df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "            except:\n",
    "                try:\n",
    "                    # Try MM/DD/YY format\n",
    "                    df['datetime'] = pd.to_datetime(df['datetime'], format='%m/%d/%y')\n",
    "                except:\n",
    "                    # If all else fails, infer format\n",
    "                    df['datetime'] = pd.to_datetime(df['datetime'], infer_datetime_format=True)\n",
    "            \n",
    "            # Set datetime as index\n",
    "            df.set_index('datetime', inplace=True)\n",
    "            \n",
    "            # Reindex to match the full time range (this will create NaN for missing dates)\n",
    "            reindexed = df.reindex(time_index)\n",
    "            \n",
    "            # Populate the dataset for this river\n",
    "            ds['Qout'].loc[dict(rivid=rivid)] = reindexed['streamflow_m^3/s'].values\n",
    "            \n",
    "            # Print progress periodically\n",
    "            if (i+1) % 100 == 0 or i+1 == len(csv_files):\n",
    "                print(f\"Processed {i+1}/{len(csv_files)} files\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "    \n",
    "    # Save to Zarr format with chunking for better performance\n",
    "    # Using reasonable chunk sizes for both dimensions\n",
    "    time_chunk_size = min(500, len(time_index))\n",
    "    rivid_chunk_size = min(500, len(rivids))\n",
    "    \n",
    "    encoding = {\n",
    "        'Qout': {\n",
    "            'chunks': (time_chunk_size, rivid_chunk_size),\n",
    "            'compressor': zarr.Blosc(cname='zstd', clevel=3),\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"Saving to Zarr format...\")\n",
    "    ds.to_zarr(output_zarr_path, encoding=encoding, mode='w')\n",
    "    \n",
    "    print(f\"Successfully created Zarr dataset at {output_zarr_path}\")\n",
    "    return ds\n",
    "\n",
    "# For memory-constrained environments, here's a chunked version\n",
    "def process_csv_files_to_zarr_memory_efficient(csv_dir, output_zarr_path):\n",
    "    \"\"\"\n",
    "    Memory-efficient version for very large datasets\n",
    "    Processes files in batches and writes to Zarr incrementally\n",
    "    \"\"\"\n",
    "    # Get list of all CSV files in the directory\n",
    "    csv_files = glob.glob(os.path.join(csv_dir, '*.csv'))\n",
    "    \n",
    "    if not csv_files:\n",
    "        raise ValueError(f\"No CSV files found in {csv_dir}\")\n",
    "    \n",
    "    print(f\"Found {len(csv_files)} CSV files\")\n",
    "    \n",
    "    # Get river IDs from filenames\n",
    "    rivids = [int(os.path.splitext(os.path.basename(file))[0]) for file in csv_files]\n",
    "    \n",
    "    # Create time range from 1940-01-01 to 2024-10-16\n",
    "    time_index = pd.date_range(start='1980-01-01', end='2023-01-01', freq='D')\n",
    "    \n",
    "    print(f\"Creating dataset with {len(rivids)} river IDs and {len(time_index)} time steps\")\n",
    "    \n",
    "    # Set up chunking for efficient I/O\n",
    "    time_chunk_size = 500\n",
    "    rivid_chunk_size = 500\n",
    "    \n",
    "    # Create a template dataset with proper metadata\n",
    "    template_ds = xr.Dataset(\n",
    "        coords={\n",
    "            'rivid': ('rivid', np.array(rivids, dtype=np.int32)),\n",
    "            'time': time_index,\n",
    "        },\n",
    "        attrs={\n",
    "            'Conventions': 'CF-1.6',\n",
    "            'comment': '',\n",
    "            'featureType': 'timeSeries',\n",
    "            'history': '',\n",
    "            'date_created': datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S+00:00\"),\n",
    "            'institution': '',\n",
    "            'references': 'https://github.com/c-h-david/rapid/, http://dx.doi.org/10.1175/2011JHM1345.1',\n",
    "            'source': 'RAPID: unknown, NOT a git repository, water inflow: /mnt/inflows/101/m3_101_20240731_20241016.nc',\n",
    "            'title': ''\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Create Qout variable with chunking and proper attributes\n",
    "    qout_data = np.full((len(time_index), len(rivids)), np.nan, dtype=np.float32)\n",
    "    template_ds['Qout'] = xr.DataArray(\n",
    "        data=qout_data,\n",
    "        dims=['time', 'rivid'],\n",
    "        coords={'time': time_index, 'rivid': rivids},\n",
    "        attrs={'long_name': 'River Discharge', 'units': 'm^3/s'}\n",
    "    )\n",
    "    \n",
    "    # Initialize Zarr store with chunking\n",
    "    encoding = {\n",
    "        'Qout': {\n",
    "            'chunks': (time_chunk_size, rivid_chunk_size),\n",
    "            'compressor': zarr.Blosc(cname='zstd', clevel=3),\n",
    "        }\n",
    "    }\n",
    "    template_ds.to_zarr(output_zarr_path, encoding=encoding, mode='w')\n",
    "    \n",
    "    # Process files in batches\n",
    "    batch_size = 100\n",
    "    num_batches = (len(csv_files) + batch_size - 1) // batch_size\n",
    "    \n",
    "    for batch_idx in range(num_batches):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = min((batch_idx + 1) * batch_size, len(csv_files))\n",
    "        batch_files = csv_files[start_idx:end_idx]\n",
    "        \n",
    "        print(f\"Processing batch {batch_idx+1}/{num_batches} ({len(batch_files)} files)\")\n",
    "        \n",
    "        # Process each file in the batch\n",
    "        for i, file_path in enumerate(batch_files):\n",
    "            try:\n",
    "                rivid = int(os.path.splitext(os.path.basename(file_path))[0])\n",
    "                rivid_idx = rivids.index(rivid)\n",
    "                \n",
    "                # Read CSV file\n",
    "                df = pd.read_csv(file_path)\n",
    "                \n",
    "                # Convert datetime column\n",
    "                try:\n",
    "                    df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "                except:\n",
    "                    try:\n",
    "                        df['datetime'] = pd.to_datetime(df['datetime'], format='%m/%d/%y')\n",
    "                    except:\n",
    "                        df['datetime'] = pd.to_datetime(df['datetime'], infer_datetime_format=True)\n",
    "                \n",
    "                # Set datetime as index and reindex to match global time index\n",
    "                df.set_index('datetime', inplace=True)\n",
    "                reindexed = df.reindex(time_index)\n",
    "                \n",
    "                # Open zarr store in append mode\n",
    "                with zarr.open(output_zarr_path, mode='a') as zstore:\n",
    "                    # Get Qout array\n",
    "                    qout_array = zstore['Qout']\n",
    "                    # Update with values for this river\n",
    "                    qout_array[:, rivid_idx] = reindexed['streamflow_m^3/s'].fillna(np.nan).values\n",
    "                \n",
    "                if (i+1) % 10 == 0:\n",
    "                    print(f\"  Processed {i+1}/{len(batch_files)} files in current batch\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_path}: {e}\")\n",
    "        \n",
    "        # Force garbage collection between batches\n",
    "        import gc\n",
    "        gc.collect()\n",
    "    \n",
    "    print(f\"Successfully created Zarr dataset at {output_zarr_path}\")\n",
    "    return xr.open_zarr(output_zarr_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Directory containing CSV files\n",
    "    csv_dir = 'path to folder that contain individual csv simulated files'\n",
    "    \n",
    "    # Path to save the Zarr file\n",
    "    output_zarr_path = 'path to folder to save the zarr file'\n",
    "    \n",
    "    # For smaller datasets (country level):\n",
    "    ds = process_csv_files_to_zarr(csv_dir, output_zarr_path)\n",
    "    \n",
    "    # For very large datasets (whole world but change the code to use whole file rather than individual files):\n",
    "    # ds = process_csv_files_to_zarr_memory_efficient(csv_dir, output_zarr_path)\n",
    "    \n",
    "    print(\"Final dataset info:\")\n",
    "    print(ds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
